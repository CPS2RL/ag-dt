{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec7844-8639-415a-89f5-d45e976c9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # pip install requests\n",
    "import pandas # pip install pandas\n",
    "import json\n",
    "\n",
    "\n",
    "def get_with_credentials(tok, uri, **kwargs):\n",
    "    # just in case we forget to put \"Token MyToken\"\n",
    "    token = tok if tok.lower().startswith(\"token\") else f\"Token {tok}\"\n",
    "    headers = {\"Authorization\": token}\n",
    "    return requests.get(uri, headers=headers, **kwargs)\n",
    "\n",
    "\n",
    "def get_readings_response(sn, start_date, end_date, **extra_kwargs_for_endpoint):\n",
    "    server = extra_kwargs_for_endpoint.get(\"server\", \"https://zentracloud.com\")\n",
    "    url = f\"{server}/api/v4/get_readings/\"\n",
    "\n",
    "    default_args = {\n",
    "        'output_format': \"df\",\n",
    "        'per_page': 100000,\n",
    "        'page_num': 1,\n",
    "        'sort_by': 'asc',\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date\n",
    "    }\n",
    "    data = {**default_args, **extra_kwargs_for_endpoint, \"device_sn\": sn}\n",
    "    tok = extra_kwargs_for_endpoint.pop(\"token\", \"\")\n",
    "    return get_with_credentials(tok, url, params=data)\n",
    "\n",
    "def get_readings_dataframe(sn,start_date,end_date,**extra_kwargs_for_endpoint):\n",
    "    res = get_readings_response(sn,start_date,end_date,**extra_kwargs_for_endpoint)\n",
    "    if(res.ok):\n",
    "        data = res.json()\n",
    "        return pandas.DataFrame(**json.loads(data[\"data\"]))\n",
    "    return res\n",
    "\n",
    "#fill in your token, device serial number and start and end dates here.\n",
    "tok = \"\"\n",
    "sn=\"\"\n",
    "start_date=\"2024-10-03 00:00:00\"\n",
    "end_date=\"2024-10-16 00:00:00\"\n",
    "\n",
    "#specify the server here.\n",
    "server=\"https://zentracloud.com\"\n",
    "df = get_readings_dataframe(sn, start_date, end_date, token=tok, server=server)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0ca07-c0bd-44d2-8dba-4cda1f7e929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = df.pivot_table(\n",
    "    index=['timestamp_utc', 'datetime'],\n",
    "    columns='measurement',\n",
    "    values='value',\n",
    "    aggfunc='first'  # Use 'first' to get the first non-null value\n",
    ").reset_index()\n",
    "\n",
    "# Optional: Flatten the multi-level column index\n",
    "pivoted_df.columns.name = None  # Remove the column index name\n",
    "pivoted_df.columns = [col if col is not None else '' for col in pivoted_df.columns]  # Handle None columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf0ca3-001e-455d-a181-cfc000906c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = pivoted_df.drop(['X-axis Level', 'Y-axis Level'], axis=1)\n",
    "pivoted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f93655-f5fb-4d54-a96c-b865e42c54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982c3e5-a279-46ec-9ef5-414061d60c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pivoted_df.copy()\n",
    "sequence_length = 96\n",
    "features = df.drop(['timestamp_utc', 'datetime' ], axis=1).values\n",
    "df = df.ffill()\n",
    "\n",
    "# Create sequences for prediction\n",
    "X_test = []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X_test.append(features[i:i+sequence_length])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Normalize the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955118c-f13f-42ca-9bcc-8f593c154d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('ResNet.keras')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc4fd7-277a-4e58-adfa-7dfcc9a2eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get the class with highest probability for each prediction\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "confidence_scores = np.max(predictions, axis=1)\n",
    "\n",
    "# Create reverse mapping for class interpretation\n",
    "class_mapping = {0: 'clean', 1: 'random', 2: 'malfunction', 3: 'drift', 4: 'bias'}\n",
    "\n",
    "# Define confidence thresholds for each class\n",
    "confidence_thresholds = {\n",
    "    'clean': 0.9,\n",
    "    'random': 0.9,\n",
    "    'malfunction': 0.95,\n",
    "    'drift': 0.95,\n",
    "    'bias': 0.95\n",
    "}\n",
    "\n",
    "# Convert numeric predictions to class labels with confidence thresholds\n",
    "predicted_labels = []\n",
    "for pred, conf in zip(predicted_classes, confidence_scores):\n",
    "    predicted_class = class_mapping[pred]\n",
    "    # Check if confidence meets the threshold for the predicted class\n",
    "    if conf >= confidence_thresholds[predicted_class]:\n",
    "        predicted_labels.append(predicted_class)\n",
    "    else:\n",
    "        predicted_labels.append('clean')  # Default to clean if confidence is too low\n",
    "\n",
    "# Print predictions and their confidence scores\n",
    "print(\"\\nSample Predictions:\")\n",
    "for label, conf in zip(predicted_labels[:50], confidence_scores[:50]):\n",
    "    print(f\"Class: {label}, Confidence: {conf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51533bee-d9e6-4998-8a36-c5d495159e53",
   "metadata": {},
   "source": [
    "# FST Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ddd438-7c94-4e64-8c75-6b7b6b83dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('FST_analysis.csv')\n",
    "\n",
    "# Print columns for verification\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences with overlap\n",
    "sequence_length = 24  \n",
    "stride = 1  \n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a86561f-c90d-4b65-8dc0-573fc3cf31b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('fst_model.keras')\n",
    "model.compile(optimizer='adam', loss='huber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcb973-e0cb-4b76-aec9-87bd1d615081",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_sequences)\n",
    "predictions = scaler_y.inverse_transform(predictions)\n",
    "\n",
    "# Print some sample predictions\n",
    "print(\"\\nSample Predictions (FST_EB values):\")\n",
    "print(predictions[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
