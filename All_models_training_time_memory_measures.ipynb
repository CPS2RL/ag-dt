{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('sunnyside_percent_25.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def measure_inference_time(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Measure inference time\n",
    "    \"\"\"\n",
    "    # Measure baseline overhead\n",
    "    c = 0\n",
    "    t1 = time.time()\n",
    "    for i in range(100):\n",
    "        x = 2\n",
    "    t1 = (time.time() - t1) / 100\n",
    "    \n",
    "    # Store timing results\n",
    "    arr = []\n",
    "    \n",
    "    # Process each test sample\n",
    "    for i in range(len(X_test[10])):\n",
    "        # Get single test sample\n",
    "        test_sample = X_test[i:i+1]  # Keep the batch dimension\n",
    "        \n",
    "        # Measure inference time with 50 iterations\n",
    "        t2 = time.time()\n",
    "        for _ in range(100):\n",
    "            _ = model.predict(test_sample, verbose=0)\n",
    "        t2 = (time.time() - t2) / 100\n",
    "        \n",
    "        # Calculate net inference time\n",
    "        inference_time = t2 - t1\n",
    "        arr.append(inference_time)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_time = np.mean(arr)\n",
    "    std_time = np.std(arr)\n",
    "    \n",
    "    print(\"\\nInference Time Statistics:\")\n",
    "    print(f\"Average inference time per sample: {avg_time:.4f} seconds\")\n",
    "    print(f\"Standard deviation: {std_time:.4f} seconds\")\n",
    "    print(f\"Min time: {min(arr):.4f} seconds\")\n",
    "    print(f\"Max time: {max(arr):.4f} seconds\")\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psutil\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "def measure_subset_memory_usage(model, X_test, start_idx=0, num_samples=100, num_runs=10):\n",
    "    \"\"\"\n",
    "    Measure memory usage for processing a subset of test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : ML model object\n",
    "        The model to evaluate\n",
    "    X_test : numpy.ndarray\n",
    "        Test data\n",
    "    start_idx : int\n",
    "        Starting index for the subset\n",
    "    num_samples : int\n",
    "        Number of samples to include in the subset\n",
    "    num_runs : int\n",
    "        Number of times to repeat the measurement for reliability\n",
    "    \"\"\"\n",
    "    memory_results = []\n",
    "    \n",
    "    # Get test subset using array indexing\n",
    "    X_subset = X_test[start_idx:start_idx + num_samples]\n",
    "    \n",
    "    # Get baseline memory\n",
    "    baseline_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    print(f\"Baseline memory: {baseline_memory:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nMeasuring memory usage for {len(X_subset)} samples ({num_runs} runs)...\")\n",
    "    \n",
    "    # Function to measure\n",
    "    def predict_subset():\n",
    "        return model.predict(X_subset)\n",
    "    \n",
    "    # Repeat measurement multiple times\n",
    "    for i in range(num_runs):\n",
    "        # Memory profiling for the subset\n",
    "        mem_usage = memory_usage(\n",
    "            (predict_subset, (), {}),\n",
    "            interval=0.005,  # Adjusted to 5ms sampling interval\n",
    "            max_iterations=1,\n",
    "            include_children=True\n",
    "        )\n",
    "        \n",
    "        # Calculate peak memory usage for this run\n",
    "        peak_memory = max(mem_usage) - baseline_memory\n",
    "        memory_results.append(peak_memory)\n",
    "        print(f\"Run {i+1}/{num_runs}: Peak memory usage: {peak_memory:.2f} MB\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    memory_stats = {\n",
    "        'mean': np.mean(memory_results),\n",
    "        'std': np.std(memory_results),\n",
    "        'min': np.min(memory_results),\n",
    "        'max': np.max(memory_results),\n",
    "        'per_sample_mean': np.mean(memory_results) / len(X_subset)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nMemory Usage Statistics (for subset):\")\n",
    "    print(f\"Subset size: {len(X_subset)} samples\")\n",
    "    print(f\"Average peak memory for subset: {memory_stats['mean']:.2f} MB\")\n",
    "    print(f\"Standard deviation: {memory_stats['std']:.2f} MB\")\n",
    "    print(f\"Min peak memory: {memory_stats['min']:.2f} MB\")\n",
    "    print(f\"Max peak memory: {memory_stats['max']:.2f} MB\")\n",
    "    print(f\"Average memory per sample: {memory_stats['per_sample_mean']:.4f} MB\")\n",
    "    \n",
    "    return {\n",
    "        'memory_results': memory_results,\n",
    "        'memory_stats': memory_stats,\n",
    "        'baseline_memory': baseline_memory,\n",
    "        'subset_size': len(X_subset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "sequence_length = 24\n",
    "\n",
    "num_samples = 20000\n",
    "num_runs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import sys\n",
    "# Add the path to the Python path\n",
    "sys.path.append('keras-tcn-master')\n",
    "\n",
    "# Import TCN\n",
    "from tcn import TCN\n",
    "\n",
    "# Load the data\n",
    "df = data.copy()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.ffill()\n",
    "\n",
    "# Encode the 'Class' column\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "df['Class'] = df['Class'].map(class_encoding)\n",
    "\n",
    "# Prepare data for TCN\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "labels = df['Class'].values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(class_encoding)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Create and train the TCN model\n",
    "tcn_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    TCN(64, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4, 8], padding='causal', \n",
    "        use_batch_norm=True, dropout_rate=0.2, return_sequences=True),\n",
    "    TCN(32, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4, 8], padding='causal', \n",
    "        use_batch_norm=True, dropout_rate=0.2, return_sequences=False),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "tcn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = tcn_model.fit(X_train, y_train_cat, validation_split=0.2,\n",
    "                    epochs= epochs, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = tcn_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   output_dict=True)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nModel Accuracy:\", f\"{accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes,\n",
    "                          target_names=list(class_encoding.keys()),\n",
    "                          digits=4))  # Just add this digits parameter\n",
    "\n",
    "print(\"Starting inference time measurement...\")\n",
    "inference_times = measure_inference_time(\n",
    "    model=tcn_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Usage example:\n",
    "print(\"Starting memory profiling for subset...\")\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=tcn_model,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs= num_runs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T19:59:17.618533Z",
     "iopub.status.busy": "2024-11-18T19:59:17.618096Z",
     "iopub.status.idle": "2024-11-18T19:59:17.623940Z",
     "shell.execute_reply": "2024-11-18T19:59:17.622710Z",
     "shell.execute_reply.started": "2024-11-18T19:59:17.618499Z"
    }
   },
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "df = data.copy()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.ffill()\n",
    "\n",
    "# Encode the 'Class' column\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "df['Class'] = df['Class'].map(class_encoding)\n",
    "\n",
    "# Prepare data for TCN\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "labels = df['Class'].values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(class_encoding)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define ResNet model\n",
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    shortcut = x\n",
    "    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def create_resnet_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 7, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 128, stride=2)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 256, stride=2)\n",
    "    x = residual_block(x, 256)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create and compile ResNet model\n",
    "resnet_model = create_resnet_model((X_train.shape[1], X_train.shape[2]), num_classes)\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = resnet_model.fit(X_train, y_train_cat, validation_split=0.2,\n",
    "                           epochs=epochs, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = resnet_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   output_dict=True)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nModel Accuracy:\", f\"{accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes,\n",
    "                          target_names=list(class_encoding.keys()),\n",
    "                          digits=4))  # Just add this digits parameter\n",
    "\n",
    "print(\"Starting inference time measurement...\")\n",
    "inference_times = measure_inference_time(\n",
    "    model=resnet_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Usage example:\n",
    "print(\"Starting memory profiling for subset...\")\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=resnet_model,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs=num_runs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "df = data.copy()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.ffill()\n",
    "\n",
    "# Encode the 'Class' column\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "df['Class'] = df['Class'].map(class_encoding)\n",
    "\n",
    "# Prepare data for LSTM\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "labels = df['Class'].values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(class_encoding)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define LSTM model\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile LSTM model\n",
    "lstm_model = create_lstm_model((X_train.shape[1], X_train.shape[2]), num_classes)\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = lstm_model.fit(X_train, y_train_cat, validation_split=0.2,\n",
    "                         epochs=epochs, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lstm_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   output_dict=True)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nModel Accuracy:\", f\"{accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes,\n",
    "                          target_names=list(class_encoding.keys()),\n",
    "                          digits=4))  # Just add this digits parameter\n",
    "\n",
    "print(\"Starting inference time measurement...\")\n",
    "inference_times = measure_inference_time(\n",
    "    model=lstm_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Optional: Visualize timing distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(inference_times, bins=30)\n",
    "plt.title('Distribution of Inference Times')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Starting memory profiling for subset...\")\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=lstm_model,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs=num_runs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "df = data.copy()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.ffill()\n",
    "\n",
    "# Encode the 'Class' column\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "df['Class'] = df['Class'].map(class_encoding)\n",
    "\n",
    "# Prepare data for TCN\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "labels = df['Class'].values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(class_encoding)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define Bi-LSTM model\n",
    "def create_bilstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile Bi-LSTM model\n",
    "bilstm_model = create_bilstm_model((X_train.shape[1], X_train.shape[2]), num_classes)\n",
    "bilstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = bilstm_model.fit(X_train, y_train_cat, validation_split=0.2,\n",
    "                           epochs=epochs, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = bilstm_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   output_dict=True)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nModel Accuracy:\", f\"{accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes,\n",
    "                          target_names=list(class_encoding.keys()),\n",
    "                          digits=4))  # Just add this digits parameter\n",
    "\n",
    "print(\"Starting inference time measurement...\")\n",
    "inference_times = measure_inference_time(\n",
    "    model=bilstm_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Optional: Visualize timing distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(inference_times, bins=30)\n",
    "plt.title('Distribution of Inference Times')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"Starting memory profiling for subset...\")\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=bilstm_model,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs=num_runs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "df = data.copy()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.ffill()\n",
    "\n",
    "# Encode the 'Class' column\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "df['Class'] = df['Class'].map(class_encoding)\n",
    "\n",
    "# Prepare data for TCN\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "labels = df['Class'].values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(class_encoding)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define GRU model\n",
    "def create_gru_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        GRU(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile GRU model\n",
    "gru_model = create_gru_model((X_train.shape[1], X_train.shape[2]), num_classes)\n",
    "gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = gru_model.fit(X_train, y_train_cat, validation_split=0.2,\n",
    "                        epochs=epochs, batch_size=128, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = gru_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   digits=4)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nModel Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "inference_times = measure_inference_time(\n",
    "    model=gru_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Optional: Visualize timing distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(inference_times, bins=30)\n",
    "plt.title('Distribution of Inference Times')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"Starting memory profiling for subset...\")\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=gru_model,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs=num_runs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess data\n",
    "df = data\n",
    "df = df.drop(columns=['timestamp_utc'])\n",
    "#df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "df = df.drop([ 'Senosor'], axis=1)\n",
    "df = df.ffill()\n",
    "\n",
    "# Prepare sequences\n",
    "sequence_length = sequence_length\n",
    "features = df.drop([ 'datetime', 'Class'], axis=1).values\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "labels = df['Class'].map(class_encoding).values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, len(class_encoding))\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, len(class_encoding))\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dff\": self.dff,\n",
    "            \"dropout\": self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_steps, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_steps = max_steps\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix once during initialization\n",
    "        position = tf.range(max_steps, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = tf.zeros((max_steps, d_model))\n",
    "        # Use scatter_nd to update sine values\n",
    "        sine_indices = tf.stack([\n",
    "            tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "            tf.tile(tf.range(0, d_model, 2), [max_steps])\n",
    "        ], axis=1)\n",
    "        sine_updates = tf.reshape(tf.sin(position * div_term), [-1])\n",
    "        pe = tf.tensor_scatter_nd_update(pe, sine_indices, sine_updates)\n",
    "        \n",
    "        # Use scatter_nd to update cosine values\n",
    "        if d_model > 1:\n",
    "            cosine_indices = tf.stack([\n",
    "                tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "                tf.tile(tf.range(1, d_model, 2), [max_steps])\n",
    "            ], axis=1)\n",
    "            cosine_updates = tf.reshape(tf.cos(position * div_term), [-1])\n",
    "            pe = tf.tensor_scatter_nd_update(pe, cosine_indices, cosine_updates)\n",
    "        \n",
    "        self.pe = pe[tf.newaxis, :, :]  # Add batch dimension\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pe[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_steps\": self.max_steps,\n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TimeSeriesTransformer(Model):\n",
    "    def __init__(self, \n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 max_seq_len,\n",
    "                 num_features,\n",
    "                 num_classes,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = layers.Dense(d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output layers\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.global_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.final_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Input projection\n",
    "        x = self.input_projection(inputs)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = self.global_pooling(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Final classification\n",
    "        return self.final_layer(x)\n",
    "\n",
    "# Model parameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 256\n",
    "max_seq_len = sequence_length\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(class_encoding)\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create and compile the model\n",
    "ts_transformer = TimeSeriesTransformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_features=num_features,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "ts_transformer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = ts_transformer.fit(\n",
    "    X_train,\n",
    "    y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "# Evaluate the model\n",
    "y_pred = ts_transformer.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_test_classes,\n",
    "    y_pred_classes,\n",
    "    average='weighted'\n",
    ")\n",
    "\n",
    "print(\"\\nTime Series Transformer Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   digits=4)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nModel Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "print(\"Starting inference time measurement...\")\n",
    "inference_times = measure_inference_time(\n",
    "    model=ts_transformer,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Optional: Visualize timing distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(inference_times, bins=30)\n",
    "plt.title('Distribution of Inference Times')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=ts_transformer,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs=num_runs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess data\n",
    "df = data.copy()\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "df = df.ffill()\n",
    "\n",
    "# Prepare sequences\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "labels = df['Class'].map(class_encoding).values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, len(class_encoding))\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, len(class_encoding))\n",
    "\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_steps, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_steps = max_steps\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix once during initialization\n",
    "        position = tf.range(max_steps, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = tf.zeros((max_steps, d_model))\n",
    "        # Use scatter_nd to update sine values\n",
    "        sine_indices = tf.stack([\n",
    "            tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "            tf.tile(tf.range(0, d_model, 2), [max_steps])\n",
    "        ], axis=1)\n",
    "        sine_updates = tf.reshape(tf.sin(position * div_term), [-1])\n",
    "        pe = tf.tensor_scatter_nd_update(pe, sine_indices, sine_updates)\n",
    "        \n",
    "        # Use scatter_nd to update cosine values\n",
    "        if d_model > 1:\n",
    "            cosine_indices = tf.stack([\n",
    "                tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "                tf.tile(tf.range(1, d_model, 2), [max_steps])\n",
    "            ], axis=1)\n",
    "            cosine_updates = tf.reshape(tf.cos(position * div_term), [-1])\n",
    "            pe = tf.tensor_scatter_nd_update(pe, cosine_indices, cosine_updates)\n",
    "        \n",
    "        self.pe = pe[tf.newaxis, :, :]  # Add batch dimension\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pe[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_steps\": self.max_steps,\n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "class ProbSparseAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, factor=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.factor = factor\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def _prob_QK(self, Q, K, sample_k):\n",
    "        B, H, L_Q, D = tf.shape(Q)[0], tf.shape(Q)[1], tf.shape(Q)[2], tf.shape(Q)[3]\n",
    "        L_K = tf.shape(K)[2]\n",
    "        \n",
    "        Q_K = tf.matmul(Q, K, transpose_b=True)\n",
    "        Q_K = Q_K / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        M = tf.math.reduce_max(Q_K, axis=-1, keepdims=True)\n",
    "        Q_K = Q_K - M\n",
    "        Q_K = tf.exp(Q_K)\n",
    "        \n",
    "        sample_size = tf.minimum(L_K, sample_k)\n",
    "        mean_attention = tf.reduce_mean(Q_K, axis=2)\n",
    "        _, indices = tf.nn.top_k(mean_attention, k=sample_size)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        Q = self.wq(inputs)\n",
    "        K = self.wk(inputs)\n",
    "        V = self.wv(inputs)\n",
    "        \n",
    "        Q = tf.reshape(Q, (batch_size, -1, self.num_heads, self.depth))\n",
    "        Q = tf.transpose(Q, perm=[0, 2, 1, 3])\n",
    "        K = tf.reshape(K, (batch_size, -1, self.num_heads, self.depth))\n",
    "        K = tf.transpose(K, perm=[0, 2, 1, 3])\n",
    "        V = tf.reshape(V, (batch_size, -1, self.num_heads, self.depth))\n",
    "        V = tf.transpose(V, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        L_K = tf.shape(K)[2]\n",
    "        sample_k = tf.cast(tf.math.log(tf.cast(L_K, tf.float32)) * self.factor, tf.int32)\n",
    "        sample_k = tf.minimum(sample_k, L_K)\n",
    "        \n",
    "        indices = self._prob_QK(Q, K, sample_k)\n",
    "        \n",
    "        batch_indices = tf.range(batch_size)[:, tf.newaxis, tf.newaxis]\n",
    "        batch_indices = tf.tile(batch_indices, [1, self.num_heads, sample_k])\n",
    "        head_indices = tf.range(self.num_heads)[tf.newaxis, :, tf.newaxis]\n",
    "        head_indices = tf.tile(head_indices, [batch_size, 1, sample_k])\n",
    "        \n",
    "        gather_indices = tf.stack([batch_indices, head_indices, indices], axis=-1)\n",
    "        \n",
    "        K_sampled = tf.gather_nd(K, gather_indices)\n",
    "        V_sampled = tf.gather_nd(V, gather_indices)\n",
    "        \n",
    "        attention_scores = tf.matmul(Q, K_sampled, transpose_b=True)\n",
    "        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        output = tf.matmul(attention_weights, V_sampled)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(output)\n",
    "\n",
    "class InformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1, factor=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout\n",
    "        self.factor = factor\n",
    "        \n",
    "        self.prob_attention = ProbSparseAttention(d_model, num_heads, factor)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.prob_attention(inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TimeSeriesInformer(Model):\n",
    "    def __init__(self, \n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 max_seq_len,\n",
    "                 num_features,\n",
    "                 num_classes,\n",
    "                 dropout_rate=0.1,\n",
    "                 factor=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.input_projection = layers.Dense(d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.informer_blocks = [\n",
    "            InformerBlock(d_model, num_heads, dff, dropout_rate, factor)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.global_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.final_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.input_projection(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for informer_block in self.informer_blocks:\n",
    "            x = informer_block(x, training=training)\n",
    "        \n",
    "        x = self.global_pooling(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        return self.final_layer(x)\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 256\n",
    "max_seq_len = sequence_length\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(class_encoding)\n",
    "dropout_rate = 0.1\n",
    "factor = 5\n",
    "\n",
    "# Create and compile the Informer model\n",
    "ts_informer = TimeSeriesInformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_features=num_features,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=dropout_rate,\n",
    "    factor=factor\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "ts_informer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = ts_informer.fit(\n",
    "    X_train,\n",
    "    y_train_cat,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TST-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = data\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "df = df.ffill()\n",
    "\n",
    "# Prepare sequences\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "labels = df['Class'].map(class_encoding).values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, len(class_encoding))\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, len(class_encoding))\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dff\": self.dff,\n",
    "            \"dropout\": self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_steps, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_steps = max_steps\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix once during initialization\n",
    "        position = tf.range(max_steps, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = tf.zeros((max_steps, d_model))\n",
    "        # Use scatter_nd to update sine values\n",
    "        sine_indices = tf.stack([\n",
    "            tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "            tf.tile(tf.range(0, d_model, 2), [max_steps])\n",
    "        ], axis=1)\n",
    "        sine_updates = tf.reshape(tf.sin(position * div_term), [-1])\n",
    "        pe = tf.tensor_scatter_nd_update(pe, sine_indices, sine_updates)\n",
    "        \n",
    "        # Use scatter_nd to update cosine values\n",
    "        if d_model > 1:\n",
    "            cosine_indices = tf.stack([\n",
    "                tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "                tf.tile(tf.range(1, d_model, 2), [max_steps])\n",
    "            ], axis=1)\n",
    "            cosine_updates = tf.reshape(tf.cos(position * div_term), [-1])\n",
    "            pe = tf.tensor_scatter_nd_update(pe, cosine_indices, cosine_updates)\n",
    "        \n",
    "        self.pe = pe[tf.newaxis, :, :]  # Add batch dimension\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pe[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_steps\": self.max_steps,\n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [Previous custom layer implementations remain the same - MultiHeadSelfAttention, TransformerBlock, PositionalEncoding]\n",
    "\n",
    "class TimeSeriesTransformerAutoencoder(Model):\n",
    "    def __init__(self, \n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 max_seq_len,\n",
    "                 num_features,\n",
    "                 num_classes,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = layers.Dense(d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Encoder transformer blocks\n",
    "        self.encoder_blocks = [\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = layers.Dense(d_model)\n",
    "        \n",
    "        # Decoder transformer blocks\n",
    "        self.decoder_blocks = [\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Reconstruction output\n",
    "        self.reconstruction_layer = layers.Dense(num_features)\n",
    "        \n",
    "        # Classification layers\n",
    "        self.global_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.classifier_dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.classifier_dense2 = layers.Dense(64, activation='relu')\n",
    "        self.classification_layer = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # Input projection and positional encoding\n",
    "        x = self.input_projection(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Encoder\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, training=training)\n",
    "        \n",
    "        # Store encoded representation\n",
    "        encoded = x\n",
    "        \n",
    "        # Classification branch\n",
    "        class_features = self.global_pooling(encoded)\n",
    "        class_features = self.classifier_dense1(class_features)\n",
    "        class_features = self.dropout(class_features, training=training)\n",
    "        class_features = self.classifier_dense2(class_features)\n",
    "        classified = self.classification_layer(class_features)\n",
    "        \n",
    "        # Decoder branch\n",
    "        decoder_features = self.bottleneck(encoded)\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            decoder_features = decoder_block(decoder_features, training=training)\n",
    "        reconstructed = self.reconstruction_layer(decoder_features)\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_output': reconstructed,\n",
    "            'classification_output': classified\n",
    "        }\n",
    "\n",
    "# Model parameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 256\n",
    "max_seq_len = sequence_length\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(class_encoding)\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create model\n",
    "tst_ae = TimeSeriesTransformerAutoencoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_features=num_features,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Create a sample input to build the model\n",
    "sample_input = tf.zeros((1, sequence_length, num_features))\n",
    "_ = tst_ae(sample_input)\n",
    "\n",
    "# Compile model\n",
    "tst_ae.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'reconstruction_output': 'mse',\n",
    "        'classification_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'reconstruction_output': 0.3,\n",
    "        'classification_output': 0.7\n",
    "    },\n",
    "    metrics={\n",
    "        'classification_output': ['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train model\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_classification_output_accuracy',\n",
    "    mode='max',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = tst_ae.fit(\n",
    "    X_train,\n",
    "    {\n",
    "        'reconstruction_output': X_train,\n",
    "        'classification_output': y_train_cat\n",
    "    },\n",
    "    validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = tst_ae.predict(X_test)\n",
    "reconstructed_sequences = predictions['reconstruction_output']\n",
    "y_pred = predictions['classification_output']\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_test_classes,\n",
    "    y_pred_classes,\n",
    "    average='weighted'\n",
    ")\n",
    "\n",
    "print(\"\\nTST-Autoencoder Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   output_dict=True)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nModel Accuracy:\", f\"{accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes,\n",
    "                          target_names=list(class_encoding.keys()),\n",
    "                          digits=4))  # Just add this digits parameter\n",
    "\n",
    "print(\"Starting inference time measurement...\")\n",
    "inference_times = measure_inference_time(\n",
    "    model=tst_ae,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Optional: Visualize timing distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(inference_times, bins=30)\n",
    "plt.title('Distribution of Inference Times')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"Starting memory profiling for subset...\")\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=tst_ae,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs=num_runs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = data\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_utc'], unit='s')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "df = df.drop(['timestamp_utc', 'Senosor'], axis=1)\n",
    "df = df.ffill()\n",
    "\n",
    "# Prepare sequences\n",
    "sequence_length = sequence_length\n",
    "features = df.drop(['timestamp', 'datetime', 'Class'], axis=1).values\n",
    "class_encoding = {'clean': 0, 'random': 1, 'malfunction': 2, 'drift': 3, 'bias': 4}\n",
    "labels = df['Class'].map(class_encoding).values\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(len(df) - sequence_length):\n",
    "    X.append(features[i:i+sequence_length])\n",
    "    y.append(labels[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, len(class_encoding))\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, len(class_encoding))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class LSTMAutoencoder(Model):\n",
    "    def __init__(self,\n",
    "                 sequence_length,\n",
    "                 num_features,\n",
    "                 num_classes,\n",
    "                 lstm_units=128,\n",
    "                 latent_dim=64,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.lstm_units = lstm_units\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_lstm1 = layers.LSTM(lstm_units, return_sequences=True)\n",
    "        self.encoder_lstm2 = layers.LSTM(lstm_units // 2, return_sequences=True)\n",
    "        self.encoder_lstm3 = layers.LSTM(latent_dim, return_sequences=True)\n",
    "        \n",
    "        # Classifier branch\n",
    "        self.global_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.classifier_dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.classifier_dense2 = layers.Dense(64, activation='relu')\n",
    "        self.classifier_output = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_lstm1 = layers.LSTM(latent_dim, return_sequences=True)\n",
    "        self.decoder_lstm2 = layers.LSTM(lstm_units // 2, return_sequences=True)\n",
    "        self.decoder_lstm3 = layers.LSTM(lstm_units, return_sequences=True)\n",
    "        self.decoder_output = layers.Dense(num_features)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Encoder\n",
    "        x = self.encoder_lstm1(inputs)\n",
    "        x = self.encoder_lstm2(x)\n",
    "        encoded = self.encoder_lstm3(x)\n",
    "        \n",
    "        # Classification branch\n",
    "        class_features = self.global_pooling(encoded)\n",
    "        class_features = self.classifier_dense1(class_features)\n",
    "        class_features = self.dropout1(class_features, training=training)\n",
    "        class_features = self.classifier_dense2(class_features)\n",
    "        classified = self.classifier_output(class_features)\n",
    "        \n",
    "        # Decoder branch\n",
    "        decoded = self.decoder_lstm1(encoded)\n",
    "        decoded = self.decoder_lstm2(decoded)\n",
    "        decoded = self.decoder_lstm3(decoded)\n",
    "        reconstructed = self.decoder_output(decoded)\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_output': reconstructed,\n",
    "            'classification_output': classified\n",
    "        }\n",
    "\n",
    "# Get number of features from the training data\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "# Create and compile LSTM-AE model\n",
    "lstm_ae = LSTMAutoencoder(\n",
    "    sequence_length=sequence_length,\n",
    "    num_features=num_features,\n",
    "    num_classes=len(class_encoding),\n",
    "    lstm_units=128,\n",
    "    latent_dim=64,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Build model with sample input\n",
    "sample_input = tf.zeros((1, sequence_length, num_features))\n",
    "_ = lstm_ae(sample_input)\n",
    "\n",
    "# Compile model\n",
    "lstm_ae.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'reconstruction_output': 'mse',\n",
    "        'classification_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'reconstruction_output': 0.3,\n",
    "        'classification_output': 0.7\n",
    "    },\n",
    "    metrics={\n",
    "        'classification_output': ['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train LSTM-AE model\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_classification_output_accuracy',\n",
    "    mode='max',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lstm_history = lstm_ae.fit(\n",
    "    X_train,\n",
    "    {\n",
    "        'reconstruction_output': X_train,\n",
    "        'classification_output': y_train_cat\n",
    "    },\n",
    "    validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate LSTM-AE model\n",
    "lstm_predictions = lstm_ae.predict(X_test)\n",
    "lstm_reconstructed = lstm_predictions['reconstruction_output']\n",
    "lstm_y_pred = lstm_predictions['classification_output']\n",
    "lstm_y_pred_classes = np.argmax(lstm_y_pred, axis=1)\n",
    "\n",
    "# Get true test classes from one-hot encoded format\n",
    "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "# Calculate metrics for LSTM-AE\n",
    "lstm_accuracy = accuracy_score(y_test_classes, lstm_y_pred_classes)\n",
    "lstm_precision, lstm_recall, lstm_f1, _ = precision_recall_fscore_support(\n",
    "    y_test_classes,\n",
    "    lstm_y_pred_classes,\n",
    "    average='weighted'\n",
    ")\n",
    "\n",
    "print(\"\\nLSTM-Autoencoder Performance:\")\n",
    "print(f\"Accuracy: {lstm_accuracy:.4f}\")\n",
    "print(f\"Precision: {lstm_precision:.4f}\")\n",
    "print(f\"Recall: {lstm_recall:.4f}\")\n",
    "print(f\"F1-score: {lstm_f1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "class_report = classification_report(y_test_classes, lstm_y_pred_classes,\n",
    "                                   target_names=list(class_encoding.keys()),\n",
    "                                   output_dict=True)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, lstm_y_pred_classes,\n",
    "                          target_names=list(class_encoding.keys()),\n",
    "                          digits=4))  # Just add this digits parameter\n",
    "\n",
    "print(\"Starting inference time measurement...\")\n",
    "inference_times = measure_inference_time(\n",
    "    model=lstm_ae,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Optional: Visualize timing distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(inference_times, bins=30)\n",
    "plt.title('Distribution of Inference Times')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"Starting memory profiling for subset...\")\n",
    "memory_metrics = measure_subset_memory_usage(\n",
    "    model=lstm_ae,\n",
    "    X_test=X_test,\n",
    "    start_idx=0,\n",
    "    num_samples=num_samples,  # First 100 samples\n",
    "    num_runs=num_runs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5897934,
     "sourceId": 9655056,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6036224,
     "sourceId": 9839814,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
