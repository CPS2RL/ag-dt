{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Union\n",
    "\n",
    "def inject_faults(X_test_orig: Union[np.ndarray, pd.DataFrame], \n",
    "                 fault_percentage: float = 0.2) -> Tuple[Union[np.ndarray, pd.DataFrame], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Inject multiple types of faults into test data.\n",
    "    \n",
    "    Args:\n",
    "        X_test_orig: Original test data (numpy array or pandas DataFrame)\n",
    "        fault_percentage: Percentage of samples to inject faults into (default: 0.2)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - Faulted test data\n",
    "        - Indices where faults were injected\n",
    "    \"\"\"\n",
    "    X_test_fault = X_test_orig.copy()\n",
    "    num_samples = len(X_test_fault)\n",
    "    num_fault_samples = int(num_samples * fault_percentage)\n",
    "    \n",
    "    # Select indices for fault injection\n",
    "    fault_indices = np.random.choice(num_samples, num_fault_samples, replace=False)    \n",
    "    for idx in fault_indices:\n",
    "        # Randomly select fault type\n",
    "        fault_type = np.random.choice(['random', 'malfunction', 'drift', 'bias'])\n",
    "        \n",
    "        if isinstance(X_test_fault, pd.DataFrame):\n",
    "            cols_to_modify = X_test_fault.columns\n",
    "        else:\n",
    "            cols_to_modify = range(X_test_fault.shape[1])\n",
    "            \n",
    "        if fault_type == 'random':\n",
    "            # Random fault with varying intensities\n",
    "            intensity = np.random.choice([1.5, 2.5])\n",
    "            if isinstance(X_test_fault, pd.DataFrame):\n",
    "                X_test_fault.loc[idx, cols_to_modify] *= (1 + intensity)\n",
    "            else:\n",
    "                X_test_fault[idx] *= (1 + intensity)\n",
    "                \n",
    "        elif fault_type == 'malfunction':\n",
    "            # Malfunction fault with noise based on data variance\n",
    "            for col in cols_to_modify:\n",
    "                if isinstance(X_test_fault, pd.DataFrame):\n",
    "                    variance = np.var(X_test_fault[col])\n",
    "                    noise =  np.random.normal(0, np.sqrt(variance)) * 3.0\n",
    "                    X_test_fault.loc[idx, col] += noise\n",
    "                else:\n",
    "                    variance = np.var(X_test_fault[:, col])\n",
    "                    noise =  np.random.normal(0, np.sqrt(variance)) * 3.0\n",
    "                    X_test_fault[idx, col] += noise\n",
    "                    \n",
    "        elif fault_type == 'drift':\n",
    "            # Drift fault with high intensity and some noise\n",
    "            intensity = np.random.choice([1, 2, 3, 4])\n",
    "            noise_intensity = 1.0\n",
    "            \n",
    "            for col in cols_to_modify:\n",
    "                if isinstance(X_test_fault, pd.DataFrame):\n",
    "                    values = X_test_fault[col].values\n",
    "                    variance = np.var(values)\n",
    "                    offset = values[0] * intensity\n",
    "                    noise = np.random.normal(0, np.sqrt(variance)) * noise_intensity\n",
    "                    X_test_fault.loc[idx, col] += noise + offset\n",
    "                else:\n",
    "                    variance = np.var(X_test_fault[:, col])\n",
    "                    offset = X_test_fault[0, col] * intensity\n",
    "                    noise = np.random.normal(0, np.sqrt(variance)) * noise_intensity\n",
    "                    X_test_fault[idx, col] += noise + offset\n",
    "                    \n",
    "        else:  # bias fault\n",
    "            # Bias fault with fixed intensity\n",
    "            intensity = 2.0\n",
    "            if isinstance(X_test_fault, pd.DataFrame):\n",
    "                for col in cols_to_modify:\n",
    "                    original_mean = np.mean(X_test_fault[col])\n",
    "                    X_test_fault.loc[idx, col] = original_mean * intensity\n",
    "            else:\n",
    "                for col in cols_to_modify:\n",
    "                    original_mean = np.mean(X_test_fault[:, col])\n",
    "                    X_test_fault[idx, col] = original_mean * intensity\n",
    "    \n",
    "    return X_test_fault, fault_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/keras-tcn/keras-tcn-master')\n",
    "\n",
    "# Import TCN\n",
    "from tcn import TCN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tcn import TCN  # Make sure to pip install keras-tcn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('/kaggle/input/fruit-surface-temperature/FST_analysis.csv')\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, features)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 48\n",
    "stride = 1\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TCN model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    TCN(64, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4, 8], padding='causal', \n",
    "        use_batch_norm=True, dropout_rate=0.2, return_sequences=True),\n",
    "    TCN(32, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4, 8], padding='causal', \n",
    "        use_batch_norm=True, dropout_rate=0.2, return_sequences=False),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Changed to 1 output unit for regression\n",
    "])\n",
    "\n",
    "# Compile model with regression-appropriate loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='huber')  # Using huber loss for robustness\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on clean test data\n",
    "y_pred_clean = model.predict(X_test)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Inject faults and evaluate\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "y_pred_fault = model.predict(X_test_fault)\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Dense, Input, Conv1D, BatchNormalization, \n",
    "                                   Activation, Add, GlobalAveragePooling1D)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define ResNet blocks\n",
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    shortcut = x\n",
    "    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def create_resnet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 7, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # ResNet blocks\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 128, stride=2)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 256, stride=2)\n",
    "    x = residual_block(x, 256)\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)  # Single output for regression\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('/kaggle/input/fruit-surface-temperature/FST_analysis.csv')\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, features)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 48\n",
    "stride = 1\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and compile ResNet model\n",
    "model = create_resnet_model((X_train.shape[1], X_train.shape[2]))\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='huber')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on clean test data\n",
    "y_pred_clean = model.predict(X_test)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Inject faults and evaluate\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "y_pred_fault = model.predict(X_test_fault)\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('/kaggle/input/fruit-surface-temperature/FST_analysis.csv')\n",
    "\n",
    "# Print columns for verification\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, features)\n",
    "\n",
    "# Use RobustScaler instead of MinMaxScaler for better handling of outliers\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences with overlap\n",
    "sequence_length = 48  # Increased sequence length\n",
    "stride = 1  # Add stride for overlapping sequences\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data with more data for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create improved LSTM model\n",
    "model = Sequential([\n",
    "    Input(shape=(sequence_length, len(features))),\n",
    "    BatchNormalization(),\n",
    "    LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    BatchNormalization(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Use custom learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='huber')  # Huber loss for robustness\n",
    "\n",
    "# Add callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train with larger batch size and more epochs\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on clean test data\n",
    "y_pred_clean = model.predict(X_test)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Inject faults and evaluate\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "y_pred_fault = model.predict(X_test_fault)\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_bilstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Single output for regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('/kaggle/input/fruit-surface-temperature/FST_analysis.csv')\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, features)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 48\n",
    "stride = 1\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and compile Bi-LSTM model\n",
    "model = create_bilstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='huber')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "# Evaluate on clean test data\n",
    "y_pred_clean = model.predict(X_test)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Inject faults and evaluate\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "y_pred_fault = model.predict(X_test_fault)\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_gru_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        GRU(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        GRU(32),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)  # Single output for regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('/kaggle/input/fruit-surface-temperature/FST_analysis.csv')\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, features)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 48\n",
    "stride = 1\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and compile GRU model\n",
    "model = create_gru_model((X_train.shape[1], X_train.shape[2]))\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='huber')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on clean test data\n",
    "y_pred_clean = model.predict(X_test)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Inject faults and evaluate\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "y_pred_fault = model.predict(X_test_fault)\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom Layer definitions (copying from your provided code)\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(output)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Modified TimeSeriesTransformer for regression\n",
    "class TimeSeriesTransformer(Model):\n",
    "    def __init__(self, \n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 max_seq_len,\n",
    "                 num_features,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = Dense(d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output layers for regression\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.global_pooling = GlobalAveragePooling1D()\n",
    "        self.final_layer = Dense(1)  # Single output for regression\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Input projection\n",
    "        x = self.input_projection(inputs)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = self.global_pooling(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Final regression output\n",
    "        return self.final_layer(x)\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('/kaggle/input/fruit-surface-temperature/FST_analysis.csv')\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, features)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 48\n",
    "stride = 1\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model parameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 256\n",
    "max_seq_len = sequence_length\n",
    "num_features = X_train.shape[2]\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create and compile the TST model\n",
    "model = TimeSeriesTransformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_features=num_features,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='huber')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate on clean test data\n",
    "y_pred_clean = model.predict(X_test)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Inject faults and evaluate\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "y_pred_fault = model.predict(X_test_fault)\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('FST_analysis.csv')\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "for col in features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df[col] = df[col].clip(Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 48\n",
    "stride = 1\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_steps, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_steps = max_steps\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix once during initialization\n",
    "        position = tf.range(max_steps, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = tf.zeros((max_steps, d_model))\n",
    "        # Use scatter_nd to update sine values\n",
    "        sine_indices = tf.stack([\n",
    "            tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "            tf.tile(tf.range(0, d_model, 2), [max_steps])\n",
    "        ], axis=1)\n",
    "        sine_updates = tf.reshape(tf.sin(position * div_term), [-1])\n",
    "        pe = tf.tensor_scatter_nd_update(pe, sine_indices, sine_updates)\n",
    "        \n",
    "        # Use scatter_nd to update cosine values\n",
    "        if d_model > 1:\n",
    "            cosine_indices = tf.stack([\n",
    "                tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "                tf.tile(tf.range(1, d_model, 2), [max_steps])\n",
    "            ], axis=1)\n",
    "            cosine_updates = tf.reshape(tf.cos(position * div_term), [-1])\n",
    "            pe = tf.tensor_scatter_nd_update(pe, cosine_indices, cosine_updates)\n",
    "        \n",
    "        self.pe = pe[tf.newaxis, :, :]  # Add batch dimension\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pe[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_steps\": self.max_steps,\n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "class ProbSparseAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, factor=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.factor = factor\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def _prob_QK(self, Q, K, sample_k):\n",
    "        B, H, L_Q, D = tf.shape(Q)[0], tf.shape(Q)[1], tf.shape(Q)[2], tf.shape(Q)[3]\n",
    "        L_K = tf.shape(K)[2]\n",
    "        \n",
    "        Q_K = tf.matmul(Q, K, transpose_b=True)\n",
    "        Q_K = Q_K / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        M = tf.math.reduce_max(Q_K, axis=-1, keepdims=True)\n",
    "        Q_K = Q_K - M\n",
    "        Q_K = tf.exp(Q_K)\n",
    "        \n",
    "        sample_size = tf.minimum(L_K, sample_k)\n",
    "        mean_attention = tf.reduce_mean(Q_K, axis=2)\n",
    "        _, indices = tf.nn.top_k(mean_attention, k=sample_size)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        Q = self.wq(inputs)\n",
    "        K = self.wk(inputs)\n",
    "        V = self.wv(inputs)\n",
    "        \n",
    "        Q = tf.reshape(Q, (batch_size, -1, self.num_heads, self.depth))\n",
    "        Q = tf.transpose(Q, perm=[0, 2, 1, 3])\n",
    "        K = tf.reshape(K, (batch_size, -1, self.num_heads, self.depth))\n",
    "        K = tf.transpose(K, perm=[0, 2, 1, 3])\n",
    "        V = tf.reshape(V, (batch_size, -1, self.num_heads, self.depth))\n",
    "        V = tf.transpose(V, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        L_K = tf.shape(K)[2]\n",
    "        sample_k = tf.cast(tf.math.log(tf.cast(L_K, tf.float32)) * self.factor, tf.int32)\n",
    "        sample_k = tf.minimum(sample_k, L_K)\n",
    "        \n",
    "        indices = self._prob_QK(Q, K, sample_k)\n",
    "        \n",
    "        batch_indices = tf.range(batch_size)[:, tf.newaxis, tf.newaxis]\n",
    "        batch_indices = tf.tile(batch_indices, [1, self.num_heads, sample_k])\n",
    "        head_indices = tf.range(self.num_heads)[tf.newaxis, :, tf.newaxis]\n",
    "        head_indices = tf.tile(head_indices, [batch_size, 1, sample_k])\n",
    "        \n",
    "        gather_indices = tf.stack([batch_indices, head_indices, indices], axis=-1)\n",
    "        \n",
    "        K_sampled = tf.gather_nd(K, gather_indices)\n",
    "        V_sampled = tf.gather_nd(V, gather_indices)\n",
    "        \n",
    "        attention_scores = tf.matmul(Q, K_sampled, transpose_b=True)\n",
    "        attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        output = tf.matmul(attention_weights, V_sampled)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(output)\n",
    "\n",
    "class InformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1, factor=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout\n",
    "        self.factor = factor\n",
    "        \n",
    "        self.prob_attention = ProbSparseAttention(d_model, num_heads, factor)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.prob_attention(inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TimeSeriesInformer(Model):\n",
    "    def __init__(self, \n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 max_seq_len,\n",
    "                 num_features,\n",
    "                 num_classes,\n",
    "                 dropout_rate=0.1,\n",
    "                 factor=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        self.input_projection = layers.Dense(d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.informer_blocks = [\n",
    "            InformerBlock(d_model, num_heads, dff, dropout_rate, factor)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.global_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.final_layer = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.input_projection(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for informer_block in self.informer_blocks:\n",
    "            x = informer_block(x, training=training)\n",
    "        \n",
    "        x = self.global_pooling(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        return self.final_layer(x)\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 256\n",
    "max_seq_len = sequence_length\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(class_encoding)\n",
    "dropout_rate = 0.1\n",
    "factor = 5\n",
    "\n",
    "# Create and compile the Informer model\n",
    "informer = TimeSeriesInformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_features=num_features,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=dropout_rate,\n",
    "    factor=factor\n",
    ")\n",
    "\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "informer.compile(optimizer=optimizer, loss='huber')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = informer.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on clean test data\n",
    "y_pred_clean = model.predict(X_test)\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Inject faults and evaluate\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "y_pred_fault = model.predict(X_test_fault)\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TST-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# [Previous class implementations remain the same]\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dff\": self.dff,\n",
    "            \"dropout\": self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_steps, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_steps = max_steps\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        position = tf.range(max_steps, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = tf.zeros((max_steps, d_model))\n",
    "        sine_indices = tf.stack([\n",
    "            tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "            tf.tile(tf.range(0, d_model, 2), [max_steps])\n",
    "        ], axis=1)\n",
    "        sine_updates = tf.reshape(tf.sin(position * div_term), [-1])\n",
    "        pe = tf.tensor_scatter_nd_update(pe, sine_indices, sine_updates)\n",
    "        \n",
    "        if d_model > 1:\n",
    "            cosine_indices = tf.stack([\n",
    "                tf.repeat(tf.range(max_steps), tf.shape(div_term)),\n",
    "                tf.tile(tf.range(1, d_model, 2), [max_steps])\n",
    "            ], axis=1)\n",
    "            cosine_updates = tf.reshape(tf.cos(position * div_term), [-1])\n",
    "            pe = tf.tensor_scatter_nd_update(pe, cosine_indices, cosine_updates)\n",
    "        \n",
    "        self.pe = pe[tf.newaxis, :, :]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pe[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"max_steps\": self.max_steps,\n",
    "            \"d_model\": self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Modified Time Series Transformer Autoencoder for Regression\n",
    "class TimeSeriesTransformerAutoencoder(Model):\n",
    "    def __init__(self, \n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 max_seq_len,\n",
    "                 num_features,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = layers.Dense(d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        \n",
    "        # Encoder transformer blocks\n",
    "        self.encoder_blocks = [\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = layers.Dense(d_model)\n",
    "        \n",
    "        # Decoder transformer blocks\n",
    "        self.decoder_blocks = [\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Reconstruction output\n",
    "        self.reconstruction_layer = layers.Dense(num_features)\n",
    "        \n",
    "        # Regression layers\n",
    "        self.global_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.regression_dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.regression_dense2 = layers.Dense(64, activation='relu')\n",
    "        self.regression_layer = layers.Dense(1)  # Single output for regression\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # Input projection and positional encoding\n",
    "        x = self.input_projection(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Encoder\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, training=training)\n",
    "        \n",
    "        # Store encoded representation\n",
    "        encoded = x\n",
    "        \n",
    "        # Regression branch\n",
    "        reg_features = self.global_pooling(encoded)\n",
    "        reg_features = self.regression_dense1(reg_features)\n",
    "        reg_features = self.dropout(reg_features, training=training)\n",
    "        reg_features = self.regression_dense2(reg_features)\n",
    "        predicted = self.regression_layer(reg_features)\n",
    "        \n",
    "        # Decoder branch\n",
    "        decoder_features = self.bottleneck(encoded)\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            decoder_features = decoder_block(decoder_features, training=training)\n",
    "        reconstructed = self.reconstruction_layer(decoder_features)\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_output': reconstructed,\n",
    "            'regression_output': predicted\n",
    "        }\n",
    "\n",
    "# Data preprocessing (same as before)\n",
    "# [Previous data preprocessing code remains the same until model creation]\n",
    "\n",
    "# Model parameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "dff = 256\n",
    "max_seq_len = sequence_length\n",
    "num_features = X_train.shape[2]\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create model\n",
    "model = TimeSeriesTransformerAutoencoder(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_features=num_features,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Compile model with combined loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        'reconstruction_output': 'mse',\n",
    "        'regression_output': 'huber'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'reconstruction_output': 0.3,\n",
    "        'regression_output': 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Changed from val_regression_output_loss\n",
    "    mode='min',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Changed from val_regression_output_loss\n",
    "    mode='min',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {\n",
    "        'reconstruction_output': X_train,\n",
    "        'regression_output': y_train\n",
    "    },\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate clean data\n",
    "predictions_clean = model.predict(X_test)\n",
    "y_pred_clean = predictions_clean['regression_output']\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Evaluate faulty data\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "predictions_fault = model.predict(X_test_fault)\n",
    "y_pred_fault = predictions_fault['regression_output']\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plotting the training history\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history.history['learning_rate'], label='Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Predictions plot\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LSTMAutoencoder(Model):\n",
    "    def __init__(self, \n",
    "                 sequence_length,\n",
    "                 num_features,\n",
    "                 lstm_units=[128, 64],\n",
    "                 latent_dim=32,\n",
    "                 dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_features = num_features\n",
    "        self.lstm_units = lstm_units\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_lstm1 = layers.LSTM(lstm_units[0], return_sequences=True)\n",
    "        self.encoder_dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.encoder_bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        self.encoder_lstm2 = layers.LSTM(lstm_units[1], return_sequences=False)\n",
    "        self.encoder_dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.encoder_bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = layers.Dense(latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_repeat = layers.RepeatVector(sequence_length)\n",
    "        \n",
    "        self.decoder_lstm1 = layers.LSTM(lstm_units[1], return_sequences=True)\n",
    "        self.decoder_dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.decoder_bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        self.decoder_lstm2 = layers.LSTM(lstm_units[0], return_sequences=True)\n",
    "        self.decoder_dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.decoder_bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        # Reconstruction output\n",
    "        self.reconstruction_layer = layers.Dense(num_features)\n",
    "        \n",
    "        # Regression layers\n",
    "        self.regression_dense1 = layers.Dense(64, activation='relu')\n",
    "        self.regression_dropout = layers.Dropout(dropout_rate)\n",
    "        self.regression_bn = layers.BatchNormalization()\n",
    "        self.regression_dense2 = layers.Dense(32, activation='relu')\n",
    "        self.regression_output = layers.Dense(1)  # Single output for regression\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Encoder\n",
    "        x = self.encoder_lstm1(inputs)\n",
    "        x = self.encoder_dropout1(x, training=training)\n",
    "        x = self.encoder_bn1(x, training=training)\n",
    "        \n",
    "        x = self.encoder_lstm2(x)\n",
    "        x = self.encoder_dropout2(x, training=training)\n",
    "        encoded = self.encoder_bn2(x, training=training)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(encoded)\n",
    "        \n",
    "        # Decoder path\n",
    "        x_decoded = self.decoder_repeat(bottleneck)\n",
    "        \n",
    "        x_decoded = self.decoder_lstm1(x_decoded)\n",
    "        x_decoded = self.decoder_dropout1(x_decoded, training=training)\n",
    "        x_decoded = self.decoder_bn1(x_decoded, training=training)\n",
    "        \n",
    "        x_decoded = self.decoder_lstm2(x_decoded)\n",
    "        x_decoded = self.decoder_dropout2(x_decoded, training=training)\n",
    "        x_decoded = self.decoder_bn2(x_decoded, training=training)\n",
    "        \n",
    "        reconstructed = self.reconstruction_layer(x_decoded)\n",
    "        \n",
    "        # Regression path\n",
    "        x_reg = self.regression_dense1(encoded)\n",
    "        x_reg = self.regression_dropout(x_reg, training=training)\n",
    "        x_reg = self.regression_bn(x_reg, training=training)\n",
    "        x_reg = self.regression_dense2(x_reg)\n",
    "        predicted = self.regression_output(x_reg)\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_output': reconstructed,\n",
    "            'regression_output': predicted\n",
    "        }\n",
    "\n",
    "# Read and preprocess data\n",
    "df = pd.read_csv('/kaggle/input/fruit-surface-temperature/FST_analysis.csv')\n",
    "\n",
    "# Combine Date and Time\n",
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Add time-based features\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "\n",
    "# Define features and target\n",
    "features = ['Air Temperature', 'Dew Point', 'Solar Radiation', 'Wind Speed', \n",
    "           'Hour', 'DayOfWeek', 'Month']\n",
    "target = 'FST_EB'\n",
    "\n",
    "# Handle missing values and outliers\n",
    "df[features] = df[features].ffill().bfill()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, features)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = RobustScaler()\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(df[features])\n",
    "y = scaler_y.fit_transform(df[[target]])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 48\n",
    "stride = 1\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "\n",
    "for i in range(0, len(df) - sequence_length, stride):\n",
    "    X_sequences.append(X[i:(i + sequence_length)])\n",
    "    y_sequences.append(y[i + sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = LSTMAutoencoder(\n",
    "    sequence_length=sequence_length,\n",
    "    num_features=len(features),\n",
    "    lstm_units=[128, 64],\n",
    "    latent_dim=32,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        'reconstruction_output': 'mse',\n",
    "        'regression_output': 'huber'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'reconstruction_output': 0.3,\n",
    "        'regression_output': 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Changed from val_regression_output_loss\n",
    "    mode='min',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Changed from val_regression_output_loss\n",
    "    mode='min',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {\n",
    "        'reconstruction_output': X_train,\n",
    "        'regression_output': y_train\n",
    "    },\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate clean data\n",
    "predictions_clean = model.predict(X_test)\n",
    "y_pred_clean = predictions_clean['regression_output']\n",
    "y_test_orig = scaler_y.inverse_transform(y_test)\n",
    "y_pred_clean_orig = scaler_y.inverse_transform(y_pred_clean)\n",
    "\n",
    "# Evaluate faulty data\n",
    "X_test_fault, fault_indices = inject_faults(X_test, fault_percentage=0.2)\n",
    "predictions_fault = model.predict(X_test_fault)\n",
    "y_pred_fault = predictions_fault['regression_output']\n",
    "y_pred_fault_orig = scaler_y.inverse_transform(y_pred_fault)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_clean = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_clean_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_clean_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_clean_orig)\n",
    "}\n",
    "\n",
    "metrics_fault = {\n",
    "    'MAE': mean_absolute_error(y_test_orig, y_pred_fault_orig),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test_orig, y_pred_fault_orig)),\n",
    "    'R2': r2_score(y_test_orig, y_pred_fault_orig)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics on Clean Test Data:\")\n",
    "for metric, value in metrics_clean.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics on Faulty Test Data:\")\n",
    "for metric, value in metrics_fault.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plotting the training history\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(history.history['learning_rate'], label='Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Predictions plot\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(y_test_orig[:100], label='Actual', alpha=0.8)\n",
    "plt.plot(y_pred_clean_orig[:100], label='Predicted (Clean)', alpha=0.8)\n",
    "plt.plot(y_pred_fault_orig[:100], label='Predicted (Faulty)', alpha=0.8)\n",
    "plt.title('Predictions (First 100 Samples)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('FST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5897934,
     "sourceId": 9655056,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6131220,
     "sourceId": 9966744,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
